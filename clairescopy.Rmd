<<<<<<< HEAD
=======
---
title: "Final Project EDA and Data Evaluation"
author: "Your Name"
date: "November 2024"
output:
  pdf_document: default
  html_document: default
---

### Question 3: Evaluating Data Quality

This question encourages you to begin the exploratory data analysis (EDA) for your final project. By addressing potential data quality issues early, you can identify and rectify problems promptly. For each important variable in your dataset, assess its quality by creating a table that includes the following:

-   **Continuous variables**:
    -   The number of non-missing observations.
    -   The number of missing observations.
    -   Measures of central tendency (e.g., mean, median).
    -   Measures of variability (e.g., standard deviation, interquartile range [IQR]).
-   **Categorical variables**:
    -   The levels of the variable.
    -   For each level:
        -   The number of non-missing observations.
        -   The number of missing observations.

### Answer: Comparing Predictive Performance of Betting Markets and Polls

Our project aims to compare the predictive accuracy of betting markets and polls in forecasting the outcome of the 2024 U.S. presidential election by state. We use the following datasets:

1.  **Betting Market Data**
    -   Sourced from a Kaggle dataset, this dataset scrapes Polymarket's 2024 election results by state and consolidates them into a CSV file ([source](https://www.kaggle.com/datasets/pbizil/polymarket-2024-us-election-state-data)).
2.  **Poll Data**
    -   Sourced from FiveThirtyEight, this dataset provides polling averages and raw data from the 2024 U.S. presidential election ([source](https://projects.fivethirtyeight.com/polls/president-general/2024/national/)).
3.  **Actual Election Results**
    -   Sourced from CBS News, this dataset reports the official 2024 presidential election results ([source](https://www.cbsnews.com/elections/2024/president/)).

#### Exploratory Data Analysis (EDA)

**Dataset 1: Betting Market Data**

The dataset provides separate files for each state, containing the probability of a Republican win under the column "Donald Trump" and a Democratic win under "Kamala Harris." These probabilities are based on the amount bet on Polymarket for each candidate. Data was aggregated by month, resulting in a final dataset of state-level probabilities for Republican and Democratic wins from April 17, 2024, to November 4, 2024. Non-missing and missing observations were quantified for each probability column. Mean probabilities and standard deviations for Republican and Democratic wins nationwide were calculated.

```{r}

```

**Dataset 2: Poll Data**

<<<<<<< HEAD
- **Organization**: The dataset contains polling data categorized by pollster, state, and candidate.
- **Processing**: 
The dataset contains average polling data by candidate, date, and their adjusted percentage of being favored. We started out by exploring how many missing and non-missing data point there were for Harris and Trump. We then aggregated data over time, such that we are left with poll percentages for Harris/Trump per state. We take the mean to find the average poll percentage for Harris and Trump nationwide. We then take the standard deviation.

```{r}
polling_data <- read.csv("data/polls_data/538_data/polls_average_2024.csv")

library(dplyr)
library(tidyverse)

```

```{r}
#isolate 2024 polling data (there are many rwos from 2020 in here with no values)
polling2024 <- polling_data %>%
  filter(cycle == 2024) %>%
  #take out the pct_trend_adjusted column (it's NA everywhere)
  select(- pct_trend_adjusted)

```

```{r}
#categorical variables - checking for missing data
# **Categorical variables**:
#     -   The levels of the variable.
#     -   For each level:
#         -   The number of non-missing observations.
#         -   The number of missing observations.

#cat variables - candidate, date, state, cycle, party

cat_vars <- c("candidate", "date", "state", "cycle", "party")

#function which calculates number of levels, and missing observations for each column

cat_eda <- function(data, vars) {
  
  levels <- numeric(length(cat_vars))
  nas <- numeric(length(cat_vars))
  non_nas <- numeric(length(cat_vars))
  
  #iterate through cat varaibles
  for (i in 1:5) {
    col <- vars[i]
    #update values appropriately
    levels[i] <- length(unique(data[[col]]))
    nas[i] <- sum(is.na(data[[col]]))
    non_nas[i] <- length(data[[col]]) - nas
  }
  
   #put results into table
   results <- data.frame(
    Variable = vars,
    Levels = levels,
    Missing = nas,
    NonMissing = non_nas)
   
   return(results)
}

cat_eda(polling2024, cat_vars)
```
Notable things - not all 50 states are represented, since we only have 30 levels for the state. Only one cycle is represented here - further analysis on this might require addition of other cycles to corroborate results. Also, there are no missing data values (Except for the column that we dropped which was entirely empty) - very clear data set... and there are 4 candidates represented, although in later terms there are only 2 as people dropped out of the race. 

Interesting things to potentially look at - could Biden's initial run be understood as a separate "mini" race within this election? Can we use that as a different case study?

```{r}
  # **Continuous variables**:
  #   -   The number of non-missing observations.
  #   -   The number of missing observations.
  #   -   Measures of central tendency (e.g., mean, median).
  #   -   Measures of variability (e.g., standard deviation, interquartile range [IQR]).

cont_vars <- c("pct_estimate", "hi", "lo")

cont_eda <- function(data, vars) {
  l <- length(cont_vars)
  mean <- numeric(l)
  median <- numeric(l)
  sd <- numeric(l)
  iqr <- numeric(l)
  nas <- numeric(l)
  non_nas <- numeric(l)

  
  #iterate through cont varaibles
  for (i in 1:l) {
    col <- vars[i]
    #update values appropriately
    x <- data[[col]]
    nas[i] <- sum(is.na(x))
    non_nas[i] <- length(x) - nas
    mean[i] <- mean(x)
    median[i] <- median(x)
    sd[i] <- sd(x)
    iqr[i] <- IQR(x)
  }
  
   #put results into table
   results <- data.frame(
    Variable = vars,
    Missing = nas,
    NonMissing = non_nas,
    Mean = mean,
    Median = median,
    IQR = iqr,
    StandardDeviation = sd)
   
   return(results)
}

cont_eda(polling2024, cont_vars)

```
```{r}
#polling information by state
polling_bystate <- polling2024 %>% 
  filter(candidate == "Trump" | candidate == "Harris") %>%
  group_by(state, candidate) %>%
  summarize(mean = mean(pct_estimate), 
          median = median(pct_estimate),
          sd = sd(pct_estimate), 
          iqr = IQR(pct_estimate))

polling_bystate
```
Things to think about - again, we have no missing values, so there are no missing values throughout this entire dataset. This gives us a good idea for around where the range hovers (i.e. there was no clear winner), which gives evidence that this is an interesting question (since if there was an obvious winner, the additional granularity of the betting data might be useless because the answer is so obvious.) It might be interesting that the range of the the highs is actually lower than the lows, but the standard deviations are all relatively similar. Looking by state also gives us some interesting information! 


Okay. now to start making visualizations!! 

```{r}
#preliminary wrangling
results <- read.csv("data/actual_results_data/state_results_2024.csv")

results <- results %>% 
  mutate(trump_win = case_when(Trump_Votes > Harris_Votes ~ 1, 
                                Trump_Votes < Harris_Votes ~ 0))

polling_bystate_rshape <- polling_bystate %>% 
  select(state, candidate, mean) %>% 
  pivot_wider(names_from = candidate, values_from = mean) %>%
  mutate(pred_twin = case_when(Trump > Harris ~ 1, 
                               Trump < Harris ~ 0))

state_polls <- filter(polling_bystate, candidate == "Trump")
#below has a few NAs because of missing cd2s in nebraska, maine, and national reuslt
polling_bystate_tr <- left_join(polling_bystate_rshape, results, by = c("state" = "State"))

matched_results <- mutate(polling_bystate_tr,
                          match = case_when(trump_win == pred_twin ~ 1, 
                                            trump_win != pred_twin ~ 0), 
                          state = tolower(state))


```

```{r}
library(maps)
us_states <- map_data("state")

us_states <- us_states %>%
  left_join(matched_results, by = c("region" = "state")) %>% 
  mutate(match = coalesce(match, 2), 
         match = as.factor(match))

ggplot(us_states, aes(long, lat, group = group, fill = match)) +
  geom_polygon(color = "white") +  # Draw state boundaries
  scale_fill_manual(
    values = c("0" = "red", "1" = "blue", "2" = "grey"),
    name = "Match",  # Legend title
    labels = c("Wrong", "Right", "NA"))  +
  labs(title = "Total Poll Accuracy in Predicting Trump Win")
```

```{r}
#lets see if its the same when we only look at the last month of polling - maybe the earlier results were not accurate? 



```

```{r}
#looking at overall trends over time 
polling2024_th <- polling2024 %>% 
  mutate(date = as.POSIXct(polling2024$date, format = "%Y-%m-%d"))  %>% 
  filter(candidate == "Trump" | candidate == "Harris") %>%
  filter(state %in% c("Arizona", "Georgia", "Michigan", "Nevada", "Pennsylvania", "Wisconsin")) %>% 
  filter(date > as.POSIXct("2024-07-21") & date < as.POSIXct("2024-11-05") )


ggplot(polling2024_th, aes(x = date, y = pct_estimate, color = candidate)) +
  geom_line() + 
  facet_wrap(~state) + 
  labs(title = "Polling Over Time by State and Candidate",
       x = "Date", y = "Percentage Estimate") + 
  scale_color_manual(values = c("Trump" = "red", "Harris" = "blue")) # Custom colors

```


okay time for some linear model making,,,,, focus on data transformations and stuff

```{r}
betting_time <- read.csv("data/filtered_data/betting_averages_time.csv")
polling_state <- read.csv("data/filtered_data/polling_averages_state.csv")
polling_time <- read.csv("data/filtered_data/polling_averages_time.csv")
results_state <- read.csv("data/filtered_data/results_averages_state.csv")

```

```{r}
#combining into one table?

betting_time1 <- betting_time %>%
  pivot_wider(names_from = Candidate, values_from = Percentage) %>%
  rename("bet_pct_trump" = "Donald Trump", "bet_pct_harris"= "Kamala Harris")


polling_time1 <- polling_time %>%
  pivot_wider(names_from = Candidate, values_from = Percentage) %>%
  rename("poll_pct_trump" = "Donald Trump", "poll_pct_harris"= "Kamala Harris")

combined <- left_join(betting_time1, polling_time1, by = c("Date", "State"))


# results_state1 <- results_state %>% 
#   pivot_wider(names_from = Candidate, values_from = Percentage) %>%
#   rename("bet_pct_trump" = "Donald Trump", "bet_pct_harris"= "Kamala Harris")
#   mutate("trump_win" = case_when(Donald Trump > Kamala HAr ~ 1, 
#                                 Trump_Votes < Harris_Votes ~ 0))

```


```{r}
combined <- read.csv("data/filtered_data/final_combined_data.csv")
install.packages("car")
library(car)

```

```{r}
#visualizations???

ggplot(combined, aes(y = Results_pct_Donald.Trump, x = Bet_pct_Donald.Trump)) +
  geom_point()

ggplot(combined, aes(y = Results_pct_Donald.Trump, x = Poll_pct_Donald.Trump)) +
  geom_point()

ggplot(combined, aes(x = Bet_pct_Donald.Trump, y = Poll_pct_Donald.Trump)) + 
  geom_point()
```

```{r}
base_model_trump <- lm(data = combined, Results_pct_Donald.Trump ~ Bet_pct_Donald.Trump + Poll_pct_Donald.Trump)

base_model_harris <- lm(data = combined, Results_pct_Kamala.Harris ~ Bet_pct_Kamala.Harris + Poll_pct_Kamala.Harris)

summary(base_model_trump)
AIC(base_model_trump)

summary(base_model_harris)
```


```{r}

#trying some transformations 

combined <- combined %>% 
  mutate(log_bet_trump = log(Bet_pct_Donald.Trump), 
         log_bet_harris = log(Bet_pct_Kamala.Harris), 
         logit_bet_trump = logit(Bet_pct_Donald.Trump), 
         logit_bet_harris = logit(Bet_pct_Kamala.Harris))

ggplot(combined, aes(x = log_bet_trump, y = Results_pct_Donald.Trump)) +
  geom_point()

ggplot(combined, aes(x = logit_bet_trump, y = Results_pct_Donald.Trump)) +
  geom_point()
```
```{r}
#new models with those transformations

lm_trump_betlogit <- lm(data = combined, Results_pct_Donald.Trump ~ logit_bet_trump + Poll_pct_Donald.Trump)


summary(lm_trump_betlogit)

lm_trump_bet <- lm(data = combined, Results_pct_Donald.Trump ~
                     Bet_pct_Donald.Trump)
lm_trump_bet_logit <- lm(data = combined, Results_pct_Donald.Trump ~
                           logit_bet_trump)

summary(lm_trump_bet)
AIC(lm_trump_bet)


summary(lm_trump_bet_logit)
AIC(lm_trump_bet_logit)


```
```{r}
install.packages(olsrr)
library(olsrr)
```


```{r}
#model selection process - best subset 
model <- lm(data = combined, Results_pct_Donald.Trump ~ 
              Poll_pct_Donald.Trump + Bet_pct_Donald.Trump
            + log_bet_trump + logit_bet_trump)

best.sub <- ols_step_best_subset(model)

best.sub$metrics[, c("predictors", "adjr", "aic", "sbic")]

#model selection - combined sequential

interceptmodel <- lm(Results_pct_Donald.Trump ~  1, data = combined)
stepwisemodel = step(interceptmodel, 
                     scope = list(lower = formula(interceptmodel), 
                                  upper = formula(model)), 
                     direction = "both", trace = 0)

formula(stepwisemodel)

stepwisemodel_aic = step(interceptmodel, 
                     scope = list(lower = formula(interceptmodel), 
                                  upper = formula(model)), 
                     direction = "both", trace = 0, k = 2)

formula(stepwisemodel_aic)
```
Both the above agree that poll pct + log_bet_trump is the best one? 

```{r}
#with interactions?
interactionmodel <- lm(Results_pct_Donald.Trump ~  
                         (Poll_pct_Donald.Trump + Bet_pct_Donald.Trump
            + log_bet_trump + logit_bet_trump)^2, data = combined)

stepwisemodel_int = step(interceptmodel, 
                     scope = list(lower = formula(interceptmodel), 
                                  upper = formula(interactionmodel)), 
                     direction = "both", trace = 0)

formula(stepwisemodel_int)
```
^^ i.e. interaction terms are not useful here? 

**Dataset 3: Election Results**

The dataset contains the number of votes for Harris and Trump, separated by state. We find the percentage who voted for Harris and Trump, the amount of missing and non-missing data, and the mean and variance. 

```{r}
```

### Bibliography

- Kaggle. *Polymarket 2024 US Election State Data*. Retrieved from: <https://www.kaggle.com/datasets/pbizil/polymarket-2024-us-election-state-data>  
- FiveThirtyEight. *2024 Presidential Polls*. Retrieved from: <https://projects.fivethirtyeight.com/polls/president-general/2024/national/>  
- CBS News. *2024 Presidential Election Results*. Retrieved from: <https://www.cbsnews.com/elections/2024/president/>
>>>>>>> 639d512893906ff6c053736690bf5d96a9acf6c8
